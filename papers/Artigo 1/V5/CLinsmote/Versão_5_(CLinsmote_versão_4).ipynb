{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aureavaleria/DataBalancing-Research/blob/main/papers/Artigo%201/V5/CLinsmote/Vers%C3%A3o_5_(CLinsmote_vers%C3%A3o_4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuLJn5A4B7XA"
      },
      "source": [
        "### ***Machine learning for predicting liver and/or lung metastasis in colorectal cancer: A retrospective study based on the SEER database***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6nNMd_ZF8r7"
      },
      "source": [
        "Este estudo prop√µe um modelo de aprendizado de m√°quina para prever o risco de met√°stase hep√°tica e/ou pulmonar em pacientes com c√¢ncer colorretal (CRC). A partir da base de dados SEER, foram extra√≠dos dados aproximadamente 53 mil pacientes com diagn√≥stico patol√≥gico de CRC entre 2010 e 2015, desenvolvendo sete modelos de algoritmos(Decision tree, Randon Forest, Naive Bayes,  KNN,XGBoost, Gradient Boosting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsC61a-EHKfA"
      },
      "source": [
        "### Parte 1:  Importa√ß√£o das Bibliotecas e Carregamento do Dataset\n",
        "\n",
        "Nesta etapa, importamos as bibliotecas necess√°rias para an√°lise e carregamos o dataset. Realizamos uma verifica√ß√£o inicial para identificar e remover valores faltantes e definimos as vari√°veis preditoras (X) e as vari√°veis alvo (y), preparando os dados para o pr√©-processamento e a modelagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6zj66AQc_z9U"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, average_precision_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Carregar o dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/aureavaleria/dataset/refs/heads/main/export.csv')\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Definir as vari√°veis preditoras e a vari√°vel alvo\n",
        "X = df[['Age recode with <1 year olds', 'Sex', 'Race recode (White, Black, Other)',\n",
        "        'Histologic Type ICD-O-3', 'Grade Recode (thru 2017)', 'Primary Site',\n",
        "        'Derived AJCC T, 7th ed (2010-2015)', 'Derived AJCC N, 7th ed (2010-2015)',\n",
        "        'CS tumor size (2004-2015)', 'CEA Pretreatment Interpretation Recode (2010+)',\n",
        "        'Tumor Deposits Recode (2010+)', 'Marital status at diagnosis']]\n",
        "\n",
        "y_liver = df['SEER Combined Mets at DX-liver (2010+)']\n",
        "y_lung = df['SEER Combined Mets at DX-lung (2010+)']\n",
        "\n",
        "y = pd.concat([y_liver, y_lung], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv7ZXmnvLghh"
      },
      "source": [
        "###Parte 2:  Prepara√ß√£o das Vari√°veis Alvo e Codifica√ß√£o de Vari√°veis Categ√≥ricas\n",
        "\n",
        "Nesta etapa, preparamos as vari√°veis alvo (y), combinando as informa√ß√µes de met√°stase hep√°tica e pulmonar em uma coluna bin√°ria para indicar a presen√ßa de met√°stase. Tamb√©m aplicamos LabelEncoder para transformar vari√°veis categ√≥ricas de X em valores num√©ricos, facilitando o uso dos dados em modelos de aprendizado de m√°quina."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "slmbLH5kLdKP",
        "outputId": "42d48093-58cc-4c24-8ba4-a08aa08bae2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho de X: 53448\n",
            "Tamanho de y: 53448\n"
          ]
        }
      ],
      "source": [
        "y = pd.concat([y_liver, y_lung], axis=1)\n",
        "\n",
        "# Fun√ß√£o para combinar as informa√ß√µes de met√°stase hep√°tica e pulmonar em uma coluna bin√°ria 'Binary Mets'\n",
        "def combine_mets_binary(row):\n",
        "    if row['SEER Combined Mets at DX-liver (2010+)'] == 'Yes' or row['SEER Combined Mets at DX-lung (2010+)'] == 'Yes':\n",
        "        return 1  # Com met√°stase\n",
        "    else:\n",
        "        return 0  # Sem met√°stase\n",
        "\n",
        "# Aplicar a fun√ß√£o para criar a nova coluna bin√°ria 'Binary Mets' em 'y'\n",
        "y['Binary Mets'] = y.apply(combine_mets_binary, axis=1)\n",
        "\n",
        "# Verificar se 'X' e 'y' t√™m o mesmo n√∫mero de amostras\n",
        "print(f\"Tamanho de X: {len(X)}\")\n",
        "print(f\"Tamanho de y: {len(y)}\")\n",
        "\n",
        "# Salvar o DataFrame 'y' em um arquivo CSV para refer√™ncia futura ou an√°lise adicional\n",
        "y.to_csv('/content/Y.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzQVE1mmMiRn"
      },
      "source": [
        "###Parte 3: Defini√ß√£o e Configura√ß√£o dos Modelos de Aprendizado de M√°quina e Valida√ß√£o Cruzada\n",
        "\n",
        "Aqui, configuramos os principais algoritmos de aprendizado de m√°quina, incluindo Decision Tree, Random Forest, SVM, Naive Bayes, KNN, XGBoost e Gradient Boosting. Cada modelo √© definido com par√¢metros espec√≠ficos para otimizar o desempenho. Em seguida, aplicamos uma valida√ß√£o cruzada estratificada com 5 divis√µes para avaliar e comparar a performance dos modelos de maneira consistente e robusta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYhfSO896ksd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
        "\n",
        "def clin_smote_balance(X, y):\n",
        "    # Imputa√ß√£o de valores ausentes apenas para colunas num√©ricas\n",
        "    num_cols = X.select_dtypes(include=[np.number]).columns\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X[num_cols] = imputer.fit_transform(X[num_cols])\n",
        "\n",
        "    # Separa√ß√£o das classes\n",
        "    X['Binary Mets'] = y\n",
        "    X_major = X[X['Binary Mets'] == 0].drop(columns=['Binary Mets'])\n",
        "    X_minor = X[X['Binary Mets'] == 1].drop(columns=['Binary Mets'])\n",
        "\n",
        "    # Perfil t√≠pico da classe majorit√°ria via KMeans para num√©ricas\n",
        "    num_cols = X_major.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    # 1. Padroniza as vari√°veis num√©ricas\n",
        "    scaler = StandardScaler()\n",
        "    X_major_num_scaled = scaler.fit_transform(X_major[num_cols])\n",
        "\n",
        "    # 2. Aplica KMeans para identificar subgrupos dentro da classe majorit√°ria\n",
        "    n_clusters = 15\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(X_major_num_scaled)\n",
        "\n",
        "    # 3. Determina os perfis t√≠picos para cada cluster\n",
        "    perfil_majoritario = []\n",
        "\n",
        "    for cluster_label in range(n_clusters):\n",
        "        perfil_cluster = {}\n",
        "        cluster_indices = (kmeans.labels_ == cluster_label)\n",
        "        X_cluster = X_major[cluster_indices]\n",
        "\n",
        "        # Perfil para vari√°veis num√©ricas\n",
        "        for col in num_cols:\n",
        "            perfil_cluster[col] = X_cluster[col].median()\n",
        "\n",
        "        # Perfil para vari√°veis categ√≥ricas\n",
        "        for col in X_major.columns:\n",
        "            if col not in num_cols:\n",
        "                if X_cluster[col].nunique() < 10:\n",
        "                    freq = X_cluster[col].value_counts(normalize=True)\n",
        "                    dominante = freq[freq >= 0.7]\n",
        "                    if not dominante.empty:\n",
        "                        perfil_cluster[col] = dominante.index[0]\n",
        "                    else:\n",
        "                        perfil_cluster[col] = X_cluster[col].mode()[0]\n",
        "                else:\n",
        "                    perfil_cluster[col] = X_cluster[col].mode()[0]\n",
        "\n",
        "        perfil_majoritario.append(perfil_cluster)\n",
        "\n",
        "    minor_status = []\n",
        "    # Classifica√ß√£o das amostras minorit√°rias\n",
        "    for _, row in X_minor.iterrows():\n",
        "        match, total = 0, 0\n",
        "\n",
        "        # --- NUM√âRICAS: compara com centroide mais pr√≥ximo\n",
        "        row_scaled = scaler.transform([row[num_cols].values])[0]\n",
        "        dists = np.linalg.norm(kmeans.cluster_centers_ - row_scaled, axis=1)\n",
        "        cluster_idx = np.argmin(dists)  # √≠ndice do cluster mais pr√≥ximo\n",
        "        centroide_prox = kmeans.cluster_centers_[cluster_idx]\n",
        "\n",
        "        # Compara√ß√£o num√©ricas\n",
        "        for i, col in enumerate(num_cols):\n",
        "            total += 1\n",
        "            if abs(row_scaled[i] - centroide_prox[i]) < 0.1:\n",
        "                match += 1\n",
        "\n",
        "        # Compara√ß√£o categ√≥ricas (usa o perfil do cluster correspondente)\n",
        "        perfil_categorico = perfil_majoritario[cluster_idx]\n",
        "        for col in perfil_categorico:\n",
        "            if col not in num_cols:\n",
        "                total += 1\n",
        "                if row[col] == perfil_categorico[col]:\n",
        "                    match += 1\n",
        "\n",
        "        percent = match / total if total > 0 else 0\n",
        "        minor_status.append('üü¢' if percent < 0.3 else 'üü°' if percent < 0.7 else 'üî¥')\n",
        "\n",
        "\n",
        "    X_minor['Status'] = minor_status\n",
        "\n",
        "\n",
        "    # Supondo que X_minor j√° tem a coluna 'Status' com os r√≥tulos üü¢üü°üî¥\n",
        "\n",
        "    # 1. Seleciona todos os verdes\n",
        "    X_minor_green = X_minor[X_minor['Status'] == 'üü¢'].drop(columns='Status')\n",
        "\n",
        "    # 1a. Remove outliers num√©ricos do grupo verde usando IQR\n",
        "    def remove_outliers_iqr(df):\n",
        "        mask = pd.Series([True]*len(df), index=df.index)\n",
        "        num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        for col in num_cols:\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower = Q1 - 1.5 * IQR\n",
        "            upper = Q3 + 1.5 * IQR\n",
        "            mask = mask & df[col].between(lower, upper)\n",
        "        return df[mask]\n",
        "\n",
        "    X_minor_green = remove_outliers_iqr(X_minor_green)\n",
        "\n",
        "\n",
        "    # 2. Seleciona uma fra√ß√£o dos intermedi√°rios (por exemplo, 60%)\n",
        "    frac_intermediarios = 0.4\n",
        "    X_minor_yellow = X_minor[X_minor['Status'] == 'üü°'].sample(frac=frac_intermediarios, random_state=42).drop(columns='Status')\n",
        "\n",
        "    # 3. Junta verdes e os intermedi√°rios escolhidos\n",
        "    X_pool_sintetico = pd.concat([X_minor_green, X_minor_yellow])\n",
        "\n",
        "    # 4. Gera as amostras sint√©ticas a partir desse pool\n",
        "    def gerar_sinteticas(df, n_amostras):\n",
        "        sinteticas = []\n",
        "        for _ in range(n_amostras):\n",
        "            base = df.sample(2, replace=True)\n",
        "            nova = {col: np.random.choice(base[col].values) for col in df.columns}\n",
        "            sinteticas.append(nova)\n",
        "        return pd.DataFrame(sinteticas)\n",
        "\n",
        "    n_sinteticas = max(0, int(0.3 * (len(X_major) - len(X_minor))))\n",
        "    X_sinteticas = gerar_sinteticas(X_pool_sintetico, n_sinteticas)\n",
        "\n",
        "    # 5. Junta tudo\n",
        "    X_major['Binary Mets'] = 0\n",
        "    X_minor['Binary Mets'] = 1\n",
        "    X_sinteticas['Binary Mets'] = 1\n",
        "\n",
        "    X_balanceado = pd.concat([X_major, X_minor, X_sinteticas], ignore_index=True)\n",
        "    y_balanceado = X_balanceado['Binary Mets']\n",
        "    X_balanceado.drop(columns=['Binary Mets', 'Status'], errors='ignore', inplace=True)\n",
        "\n",
        "    return X_balanceado, y_balanceado\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IikJrwJzYznn",
        "outputId": "f48ae0a2-fe71-4bf5-9d9b-98b60507fe98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-3939197387.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[cat_cols] = X[cat_cols].astype('category')\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "cat_cols = X.select_dtypes(include='object').columns\n",
        "X[cat_cols] = X[cat_cols].astype('category')\n",
        "\n",
        "# Defina seus modelos:\n",
        "models = {\n",
        "    \"LightGBM\": lgb.LGBMClassifier()\n",
        "    # ... outros modelos se desejar (mas s√≥ LightGBM aceita direto!)\n",
        "}\n",
        "\n",
        "smote_techniques = {\n",
        "    \"clin_smote_balance\": clin_smote_balance\n",
        "}\n",
        "\n",
        "\n",
        "# Configura√ß√£o da valida√ß√£o cruzada estratificada com 5 divis√µes (folds)\n",
        "# Isso garante que a propor√ß√£o de classes seja mantida em cada divis√£o, e o shuffle embaralha os dados antes de dividir\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfbK68vJNOEX"
      },
      "source": [
        "### Parte 4: Avalia√ß√£o e Compara√ß√£o dos Modelos de Aprendizado de M√°quina em Conjuntos de Treino, Valida√ß√£o e Teste\n",
        "\n",
        "\n",
        "Este bloco de c√≥digo implementa a valida√ß√£o cruzada para treinar e avaliar os modelos de aprendizado de m√°quina definidos no pipeline. Ele utiliza a t√©cnica de K-Fold Cross-Validation para dividir os dados em m√∫ltiplos folds, garantindo uma avalia√ß√£o robusta do desempenho dos modelos. Durante cada fold, os dados de treinamento s√£o balanceados utilizando o SMOTE e escalados com o StandardScaler. M√©tricas de desempenho, como precis√£o, recall, F1-Score, especificidade, AUC-ROC e AUPR, s√£o calculadas tanto para o conjunto de treinamento quanto para o conjunto de teste. Al√©m disso, visualiza√ß√µes como matrizes de confus√£o e curvas ROC e Precis√£o-Recall s√£o geradas. Ao final, as m√©tricas m√©dias de todos os folds s√£o compiladas para compara√ß√£o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wun4E2DYtwQZ",
        "outputId": "8cdb124c-94c6-4ae4-ec3c-23196ccc498e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aplicando clin_smote_balance com LightGBM\n",
            "[LightGBM] [Info] Number of positive: 15357, number of negative: 36433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019952 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 278\n",
            "[LightGBM] [Info] Number of data points in the train set: 51790, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.296524 -> initscore=-0.863904\n",
            "[LightGBM] [Info] Start training from score -0.863904\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 15357, number of negative: 36433\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007247 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 278\n",
            "[LightGBM] [Info] Number of data points in the train set: 51790, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.296524 -> initscore=-0.863904\n",
            "[LightGBM] [Info] Start training from score -0.863904\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Melhores par√¢metros LightGBM nesta rodada: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 9, 'min_child_samples': 20, 'n_estimators': 500, 'num_leaves': 63, 'subsample': 0.8}\n",
            "[LightGBM] [Info] Number of positive: 15357, number of negative: 36433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010756 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 279\n",
            "[LightGBM] [Info] Number of data points in the train set: 51790, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.296524 -> initscore=-0.863904\n",
            "[LightGBM] [Info] Start training from score -0.863904\n",
            "[LightGBM] [Info] Number of positive: 15357, number of negative: 36433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010793 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 279\n",
            "[LightGBM] [Info] Number of data points in the train set: 51790, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.296524 -> initscore=-0.863904\n",
            "[LightGBM] [Info] Start training from score -0.863904\n",
            "Melhores par√¢metros LightGBM nesta rodada: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 9, 'min_child_samples': 10, 'n_estimators': 500, 'num_leaves': 63, 'subsample': 0.8}\n",
            "[LightGBM] [Info] Number of positive: 15357, number of negative: 36434\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007146 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 273\n",
            "[LightGBM] [Info] Number of data points in the train set: 51791, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.296519 -> initscore=-0.863931\n",
            "[LightGBM] [Info] Start training from score -0.863931\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 15357, number of negative: 36434\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010383 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 273\n",
            "[LightGBM] [Info] Number of data points in the train set: 51791, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.296519 -> initscore=-0.863931\n",
            "[LightGBM] [Info] Start training from score -0.863931\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Melhores par√¢metros LightGBM nesta rodada: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'min_child_samples': 10, 'n_estimators': 500, 'num_leaves': 63, 'subsample': 0.8}\n",
            "[LightGBM] [Info] Number of positive: 15357, number of negative: 36434\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007215 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 273\n",
            "[LightGBM] [Info] Number of data points in the train set: 51791, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.296519 -> initscore=-0.863931\n",
            "[LightGBM] [Info] Start training from score -0.863931\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 15357, number of negative: 36434\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011098 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 273\n",
            "[LightGBM] [Info] Number of data points in the train set: 51791, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.296519 -> initscore=-0.863931\n",
            "[LightGBM] [Info] Start training from score -0.863931\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Melhores par√¢metros LightGBM nesta rodada: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 9, 'min_child_samples': 10, 'n_estimators': 500, 'num_leaves': 63, 'subsample': 0.8}\n",
            "[LightGBM] [Info] Number of positive: 15357, number of negative: 36434\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005180 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 276\n",
            "[LightGBM] [Info] Number of data points in the train set: 51791, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.296519 -> initscore=-0.863931\n",
            "[LightGBM] [Info] Start training from score -0.863931\n",
            "[LightGBM] [Info] Number of positive: 15357, number of negative: 36434\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008464 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 276\n",
            "[LightGBM] [Info] Number of data points in the train set: 51791, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.296519 -> initscore=-0.863931\n",
            "[LightGBM] [Info] Start training from score -0.863931\n",
            "Melhores par√¢metros LightGBM nesta rodada: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 9, 'min_child_samples': 10, 'n_estimators': 500, 'num_leaves': 63, 'subsample': 0.8}\n",
            "Teste - Accuracy: 0.8674, AUC: 0.8780, Precision: 0.5481, Recall: 0.5916, F1-score: 0.5690\n",
            "Treinamento - Accuracy: 0.9218, AUC: 0.9713, Precision: 0.8913, Recall: 0.8384, F1-score: 0.8640\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 300, 500],     # tente menos, aumente s√≥ se precisar\n",
        "    'learning_rate': [0.01, 0.05, 0.1], # valores intermedi√°rios\n",
        "    'num_leaves': [15, 31, 63],         # n√£o exagere no come√ßo!\n",
        "    'max_depth': [5, 7, 9],             # evite -1 por ora\n",
        "    'min_child_samples': [10, 20],      # n√£o ponha valores altos demais\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "\n",
        "roc_curves_test = {smote_name: {} for smote_name in smote_techniques.keys()}\n",
        "roc_curves_train = {smote_name: {} for smote_name in smote_techniques.keys()}\n",
        "results_table_test = []\n",
        "results_table_train = []\n",
        "\n",
        "for smote_name, smote in smote_techniques.items():\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\nAplicando {smote_name} com {model_name}\")\n",
        "        mean_fpr = np.linspace(0, 1, 100)\n",
        "        tprs_test, aucs_test = [], []\n",
        "        accuracies_test, precisions_test, recalls_test, f1_scores_test = [], [], [], []\n",
        "        tprs_train, aucs_train = [], []\n",
        "        accuracies_train, precisions_train, recalls_train, f1_scores_train = [], [], [], []\n",
        "\n",
        "        for train_index, test_index in kf.split(X, y['Binary Mets']):\n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y['Binary Mets'].iloc[train_index], y['Binary Mets'].iloc[test_index]\n",
        "\n",
        "            # Balanceamento com RandomOverSampler (funciona com categ√≥ricos)\n",
        "            if smote_name == 'clin_smote_balance':\n",
        "                X_train_res, y_train_res = smote(X_train.copy(), y_train.copy())\n",
        "            else:\n",
        "                X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "            for col in X_train_res.select_dtypes(include='object').columns:\n",
        "                X_train_res[col] = X_train_res[col].astype('category')\n",
        "            for col in X_test.select_dtypes(include='object').columns:\n",
        "                X_test[col] = X_test[col].astype('category')\n",
        "\n",
        "            def clean_column_names(df):\n",
        "                df.columns = [\n",
        "                    re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in df.columns\n",
        "                ]\n",
        "                return df\n",
        "\n",
        "            # Limpar nomes das colunas dos datasets usados no fit e predict\n",
        "            X_train_res = clean_column_names(X_train_res)\n",
        "            X_test = clean_column_names(X_test)\n",
        "\n",
        "            # Treinamento e avalia√ß√£o\n",
        "            # S√≥ fa√ßa GridSearch para o LightGBM\n",
        "            if model_name == \"LightGBM\":\n",
        "                gs = GridSearchCV(model, param_grid, scoring='f1', cv=3, n_jobs=-1)\n",
        "                gs.fit(X_train_res, y_train_res, categorical_feature='auto')\n",
        "                best_model = gs.best_estimator_\n",
        "                if model_name == \"LightGBM\":\n",
        "                    gs = GridSearchCV(model, param_grid, scoring='f1', cv=3, n_jobs=-1)\n",
        "                    gs.fit(X_train_res, y_train_res, categorical_feature='auto')\n",
        "                    best_model = gs.best_estimator_\n",
        "                    print(f\"Melhores par√¢metros LightGBM nesta rodada: {gs.best_params_}\")\n",
        "                else:\n",
        "                    best_model = model\n",
        "                    best_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "            else:\n",
        "                best_model = model\n",
        "                best_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "            y_pred_test = best_model.predict(X_test)\n",
        "            y_pred_proba_test = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "            fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_proba_test)\n",
        "            interp_tpr_test = np.interp(mean_fpr, fpr_test, tpr_test)\n",
        "            interp_tpr_test[0] = 0.0\n",
        "            tprs_test.append(interp_tpr_test)\n",
        "            aucs_test.append(auc(fpr_test, tpr_test))\n",
        "            accuracies_test.append(accuracy_score(y_test, y_pred_test))\n",
        "            precisions_test.append(precision_score(y_test, y_pred_test))\n",
        "            recalls_test.append(recall_score(y_test, y_pred_test))\n",
        "            f1_scores_test.append(f1_score(y_test, y_pred_test))\n",
        "\n",
        "            y_pred_train = best_model.predict(X_train_res)\n",
        "            y_pred_proba_train = best_model.predict_proba(X_train_res)[:, 1]\n",
        "\n",
        "            fpr_train, tpr_train, _ = roc_curve(y_train_res, y_pred_proba_train)\n",
        "            interp_tpr_train = np.interp(mean_fpr, fpr_train, tpr_train)\n",
        "            interp_tpr_train[0] = 0.0\n",
        "            tprs_train.append(interp_tpr_train)\n",
        "            aucs_train.append(auc(fpr_train, tpr_train))\n",
        "            accuracies_train.append(accuracy_score(y_train_res, y_pred_train))\n",
        "            precisions_train.append(precision_score(y_train_res, y_pred_train))\n",
        "            recalls_train.append(recall_score(y_train_res, y_pred_train))\n",
        "            f1_scores_train.append(f1_score(y_train_res, y_pred_train))\n",
        "\n",
        "        mean_accuracy_test = np.mean(accuracies_test)\n",
        "        mean_auc_test = np.mean(aucs_test)\n",
        "        mean_precision_test = np.mean(precisions_test)\n",
        "        mean_recall_test = np.mean(recalls_test)\n",
        "        mean_f1_test = np.mean(f1_scores_test)\n",
        "        mean_accuracy_train = np.mean(accuracies_train)\n",
        "        mean_auc_train = np.mean(aucs_train)\n",
        "        mean_precision_train = np.mean(precisions_train)\n",
        "        mean_recall_train = np.mean(recalls_train)\n",
        "        mean_f1_train = np.mean(f1_scores_train)\n",
        "\n",
        "        print(f\"Teste - Accuracy: {mean_accuracy_test:.4f}, AUC: {mean_auc_test:.4f}, Precision: {mean_precision_test:.4f}, Recall: {mean_recall_test:.4f}, F1-score: {mean_f1_test:.4f}\")\n",
        "        print(f\"Treinamento - Accuracy: {mean_accuracy_train:.4f}, AUC: {mean_auc_train:.4f}, Precision: {mean_precision_train:.4f}, Recall: {mean_recall_train:.4f}, F1-score: {mean_f1_train:.4f}\")\n",
        "\n",
        "        results_table_test.append({\n",
        "            \"SMOTE Technique\": smote_name,\n",
        "            \"Model\": model_name,\n",
        "            \"Accuracy\": mean_accuracy_test,\n",
        "            \"AUC\": mean_auc_test,\n",
        "            \"Precision\": mean_precision_test,\n",
        "            \"Recall rate\": mean_recall_test,\n",
        "            \"F1-score\": mean_f1_test\n",
        "        })\n",
        "        results_table_train.append({\n",
        "            \"SMOTE Technique\": smote_name,\n",
        "            \"Model\": model_name,\n",
        "            \"Accuracy\": mean_accuracy_train,\n",
        "            \"AUC\": mean_auc_train,\n",
        "            \"Precision\": mean_precision_train,\n",
        "            \"Recall rate\": mean_recall_train,\n",
        "            \"F1-score\": mean_f1_train\n",
        "        })\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnqSkLsqzEavIVbIjv3tUR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}