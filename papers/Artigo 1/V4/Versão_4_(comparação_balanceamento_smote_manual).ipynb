{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aureavaleria/DataBalancing-Research/blob/main/papers/Artigo%201/V4/Vers%C3%A3o_4_(compara%C3%A7%C3%A3o_balanceamento_smote_manual).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuLJn5A4B7XA"
      },
      "source": [
        "### ***Machine learning for predicting liver and/or lung metastasis in colorectal cancer: A retrospective study based on the SEER database***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6nNMd_ZF8r7"
      },
      "source": [
        "Este estudo propõe um modelo de aprendizado de máquina para prever o risco de metástase hepática e/ou pulmonar em pacientes com câncer colorretal (CRC). A partir da base de dados SEER, foram extraídos dados aproximadamente 53 mil pacientes com diagnóstico patológico de CRC entre 2010 e 2015, desenvolvendo sete modelos de algoritmos(Decision tree, Randon Forest, Naive Bayes,  KNN,XGBoost, Gradient Boosting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsC61a-EHKfA"
      },
      "source": [
        "### Parte 1:  Importação das Bibliotecas e Carregamento do Dataset\n",
        "\n",
        "Nesta etapa, importamos as bibliotecas necessárias para análise e carregamos o dataset. Realizamos uma verificação inicial para identificar e remover valores faltantes e definimos as variáveis preditoras (X) e as variáveis alvo (y), preparando os dados para o pré-processamento e a modelagem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "id": "6zj66AQc_z9U"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, average_precision_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Carregar o dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/aureavaleria/dataset/refs/heads/main/export.csv')\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Definir as variáveis preditoras e a variável alvo\n",
        "X = df[['Age recode with <1 year olds', 'Sex', 'Race recode (White, Black, Other)',\n",
        "        'Histologic Type ICD-O-3', 'Grade Recode (thru 2017)', 'Primary Site',\n",
        "        'Derived AJCC T, 7th ed (2010-2015)', 'Derived AJCC N, 7th ed (2010-2015)',\n",
        "        'CS tumor size (2004-2015)', 'CEA Pretreatment Interpretation Recode (2010+)',\n",
        "        'Tumor Deposits Recode (2010+)', 'Marital status at diagnosis']]\n",
        "\n",
        "y_liver = df['SEER Combined Mets at DX-liver (2010+)']\n",
        "y_lung = df['SEER Combined Mets at DX-lung (2010+)']\n",
        "\n",
        "y = pd.concat([y_liver, y_lung], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv7ZXmnvLghh"
      },
      "source": [
        "###Parte 2:  Preparação das Variáveis Alvo e Codificação de Variáveis Categóricas\n",
        "\n",
        "Nesta etapa, preparamos as variáveis alvo (y), combinando as informações de metástase hepática e pulmonar em uma coluna binária para indicar a presença de metástase. Também aplicamos LabelEncoder para transformar variáveis categóricas de X em valores numéricos, facilitando o uso dos dados em modelos de aprendizado de máquina."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "slmbLH5kLdKP",
        "outputId": "0c98a8af-918d-4273-8cbc-adf38b3d0df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-9761f72aaee8>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = LabelEncoder().fit_transform(X[col])\n",
            "<ipython-input-18-9761f72aaee8>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = LabelEncoder().fit_transform(X[col])\n",
            "<ipython-input-18-9761f72aaee8>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = LabelEncoder().fit_transform(X[col])\n",
            "<ipython-input-18-9761f72aaee8>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = LabelEncoder().fit_transform(X[col])\n",
            "<ipython-input-18-9761f72aaee8>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = LabelEncoder().fit_transform(X[col])\n",
            "<ipython-input-18-9761f72aaee8>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = LabelEncoder().fit_transform(X[col])\n",
            "<ipython-input-18-9761f72aaee8>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = LabelEncoder().fit_transform(X[col])\n",
            "<ipython-input-18-9761f72aaee8>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = LabelEncoder().fit_transform(X[col])\n",
            "<ipython-input-18-9761f72aaee8>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = LabelEncoder().fit_transform(X[col])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho de X: 53448\n",
            "Tamanho de y: 53448\n"
          ]
        }
      ],
      "source": [
        "y = pd.concat([y_liver, y_lung], axis=1)\n",
        "\n",
        "# Aplicar codificação a variáveis categóricas em 'X' usando LabelEncoder, para prepará-las para o modelo\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':  # Verifica se a coluna é categórica (strings)\n",
        "        X[col] = LabelEncoder().fit_transform(X[col])\n",
        "\n",
        "# Função para combinar as informações de metástase hepática e pulmonar em uma coluna binária 'Binary Mets'\n",
        "def combine_mets_binary(row):\n",
        "    if row['SEER Combined Mets at DX-liver (2010+)'] == 'Yes' or row['SEER Combined Mets at DX-lung (2010+)'] == 'Yes':\n",
        "        return 1  # Com metástase\n",
        "    else:\n",
        "        return 0  # Sem metástase\n",
        "\n",
        "# Aplicar a função para criar a nova coluna binária 'Binary Mets' em 'y'\n",
        "y['Binary Mets'] = y.apply(combine_mets_binary, axis=1)\n",
        "\n",
        "# Verificar se 'X' e 'y' têm o mesmo número de amostras\n",
        "print(f\"Tamanho de X: {len(X)}\")\n",
        "print(f\"Tamanho de y: {len(y)}\")\n",
        "\n",
        "# Salvar o DataFrame 'y' em um arquivo CSV para referência futura ou análise adicional\n",
        "y.to_csv('/content/Y.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Parte 3: implementação manual do smote"
      ],
      "metadata": {
        "id": "Gzot_dMBZVXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "# Função para implementar o SMOTE manualmente\n",
        "def manual_smote(X, y, minority_class, n_neighbors=5, n_samples=None):\n",
        "    \"\"\"\n",
        "    Implementa o SMOTE manualmente para gerar amostras sintéticas.\n",
        "\n",
        "    Parâmetros:\n",
        "        X (numpy array): Features dos dados.\n",
        "        y (numpy array): Rótulos dos dados.\n",
        "        minority_class: Classe minoritária no dataset.\n",
        "        n_neighbors (int): Número de vizinhos a considerar para a geração.\n",
        "        n_samples (int): Número de amostras sintéticas a gerar (opcional).\n",
        "\n",
        "    Retorna:\n",
        "        X_resampled, y_resampled: Dados balanceados com amostras sintéticas.\n",
        "    \"\"\"\n",
        "    # Identificar os índices das amostras da classe minoritária\n",
        "    minority_indices = np.where(y == minority_class)[0]\n",
        "    X_minority = X[minority_indices]\n",
        "\n",
        "    # Número de amostras sintéticas a gerar\n",
        "    if n_samples is None:\n",
        "        n_samples = len(X) - 2 * len(X_minority)\n",
        "\n",
        "    # Encontrar os vizinhos mais próximos\n",
        "    nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(X_minority)\n",
        "    neighbors = nbrs.kneighbors(X_minority, return_distance=False)\n",
        "\n",
        "    # Gerar amostras sintéticas\n",
        "    synthetic_samples = []\n",
        "    for _ in range(n_samples):\n",
        "        idx = np.random.choice(len(X_minority))\n",
        "        neighbor_idx = np.random.choice(neighbors[idx, 1:])  # Escolher um vizinho aleatoriamente\n",
        "\n",
        "        point_1 = X_minority[idx]\n",
        "        point_2 = X_minority[neighbor_idx]\n",
        "\n",
        "        # Criar um ponto sintético\n",
        "        synthetic_sample = point_1 + np.random.rand() * (point_2 - point_1)\n",
        "        synthetic_samples.append(synthetic_sample)\n",
        "\n",
        "    # Adicionar as amostras sintéticas aos dados originais\n",
        "    X_synthetic = np.array(synthetic_samples)\n",
        "    y_synthetic = np.full(len(synthetic_samples), minority_class)\n",
        "\n",
        "    X_resampled = np.vstack([X, X_synthetic])\n",
        "    y_resampled = np.hstack([y, y_synthetic])\n",
        "\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "# Wrapper para chamar a função manual de SMOTE\n",
        "def smote_wrapper(X, y, minority_class=1, n_neighbors=5, n_samples=500):\n",
        "    return manual_smote(X, y, minority_class=minority_class, n_neighbors=n_neighbors, n_samples=n_samples)\n",
        "\n",
        "# Lista de técnicas de balanceamento\n",
        "smote_techniques = {\n",
        "    \"SMOTE\": smote_wrapper,\n",
        "}"
      ],
      "metadata": {
        "id": "nB4N1Ld4ZOCl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzQVE1mmMiRn"
      },
      "source": [
        "###Parte 4: Definição e Configuração dos Modelos de Aprendizado de Máquina e Validação Cruzada\n",
        "\n",
        "Aqui, configuramos os principais algoritmos de aprendizado de máquina, incluindo Decision Tree, Random Forest, SVM, Naive Bayes, KNN, XGBoost e Gradient Boosting. Cada modelo é definido com parâmetros específicos para otimizar o desempenho. Em seguida, aplicamos uma validação cruzada estratificada com 5 divisões para avaliar e comparar a performance dos modelos de maneira consistente e robusta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IikJrwJzYznn"
      },
      "outputs": [],
      "source": [
        "# Definição dos modelos de aprendizado de máquina com hiperparâmetros ajustados\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(\n",
        "        criterion='gini',\n",
        "        max_depth=5,\n",
        "        min_samples_leaf=10,\n",
        "        min_samples_split=2,\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"Random Forest\": RandomForestClassifier(\n",
        "        bootstrap=True,\n",
        "        criterion='entropy',\n",
        "        max_depth=15,\n",
        "        min_samples_leaf=5,\n",
        "        min_samples_split=2,\n",
        "        n_estimators=300,\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"SVM\": SVC(\n",
        "        kernel='poly',\n",
        "        gamma='scale',\n",
        "        degree=3,\n",
        "        C=10,\n",
        "        probability=True,\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"KNN\": KNeighborsClassifier(\n",
        "        leaf_size=20,\n",
        "        metric='manhattan',\n",
        "        n_neighbors=11,\n",
        "        weights='uniform'\n",
        "    ),\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        colsample_bytree=0.6,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=6,\n",
        "        n_estimators=100,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=10.0,\n",
        "        subsample=1.0,\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
        "        max_depth=3,\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        min_samples_leaf=5,\n",
        "        min_samples_split=2,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Configuração da validação cruzada estratificada com 5 divisões (folds)\n",
        "# Isso garante que a proporção de classes seja mantida em cada divisão, e o shuffle embaralha os dados antes de dividir\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfbK68vJNOEX"
      },
      "source": [
        "### Parte 5: Avaliação e Comparação dos Modelos de Aprendizado de Máquina em Conjuntos de Treino, Validação e Teste\n",
        "\n",
        "\n",
        "Este bloco de código implementa a validação cruzada para treinar e avaliar os modelos de aprendizado de máquina definidos no pipeline. Ele utiliza a técnica de K-Fold Cross-Validation para dividir os dados em múltiplos folds, garantindo uma avaliação robusta do desempenho dos modelos. Durante cada fold, os dados de treinamento são balanceados utilizando o SMOTE e escalados com o StandardScaler. Métricas de desempenho, como precisão, recall, F1-Score, especificidade, AUC-ROC e AUPR, são calculadas tanto para o conjunto de treinamento quanto para o conjunto de teste. Além disso, visualizações como matrizes de confusão e curvas ROC e Precisão-Recall são geradas. Ao final, as métricas médias de todos os folds são compiladas para comparação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zncc2wo0-3bq",
        "outputId": "f79f890c-6327-43ad-f9b2-79d505e111f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aplicando SMOTE com Decision Tree\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aplicando SMOTE com Random Forest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aplicando SMOTE com SVM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aplicando SMOTE com Naive Bayes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aplicando SMOTE com KNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aplicando SMOTE com XGBoost\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aplicando SMOTE com Gradient Boosting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resultados Comparativos:\n",
            "  SMOTE Technique              Model  F1-Score   AUC-ROC    AUC-PR\n",
            "0           SMOTE      Decision Tree  0.494546  0.858451  0.517795\n",
            "1           SMOTE      Random Forest  0.500735  0.883327  0.617526\n",
            "2           SMOTE                SVM  0.228289  0.738636  0.431222\n",
            "3           SMOTE        Naive Bayes  0.385142  0.775643  0.379646\n",
            "4           SMOTE                KNN  0.437395  0.821090  0.481009\n",
            "5           SMOTE            XGBoost  0.524243  0.887650  0.625622\n",
            "6           SMOTE  Gradient Boosting  0.522651  0.885747  0.620403\n",
            "Resultados salvos em 'comparacao_smote_resultados.csv'\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "# Iteração entre técnicas de balanceamento e modelos\n",
        "for smote_name, smote in smote_techniques.items():\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\nAplicando {smote_name} com {model_name}\")\n",
        "        fold_metrics = []\n",
        "\n",
        "        for train_index, test_index in kf.split(X, y['Binary Mets']):\n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y['Binary Mets'].iloc[train_index], y['Binary Mets'].iloc[test_index]\n",
        "\n",
        "            # Converter para arrays NumPy\n",
        "            X_train_np = X_train.values if hasattr(X_train, 'values') else X_train\n",
        "            y_train_np = y_train.values if hasattr(y_train, 'values') else y_train\n",
        "\n",
        "            # Aplicar o SMOTE manual\n",
        "            X_train_res, y_train_res = smote(X_train_np, y_train_np, minority_class=1, n_neighbors=10, n_samples=500)\n",
        "\n",
        "            # Normalizar os dados\n",
        "            scaler = StandardScaler()\n",
        "            X_train_res = scaler.fit_transform(X_train_res)\n",
        "            X_test = scaler.transform(X_test)\n",
        "\n",
        "            # Treinar o modelo\n",
        "            model.fit(X_train_res, y_train_res)\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "            # Avaliar as métricas\n",
        "            f1 = f1_score(y_test, y_pred)\n",
        "            auc = roc_auc_score(y_test, y_pred_proba)\n",
        "            auc_pr = average_precision_score(y_test, y_pred_proba)\n",
        "            fold_metrics.append((f1, auc, auc_pr))\n",
        "\n",
        "        # Métricas médias\n",
        "        avg_f1, avg_auc, avg_auc_pr = np.mean(fold_metrics, axis=0)\n",
        "        results.append({\n",
        "            \"SMOTE Technique\": smote_name,\n",
        "            \"Model\": model_name,\n",
        "            \"F1-Score\": avg_f1,\n",
        "            \"AUC-ROC\": avg_auc,\n",
        "            \"AUC-PR\": avg_auc_pr\n",
        "        })\n",
        "\n",
        "# Tabela de resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nResultados Comparativos:\")\n",
        "print(results_df)\n",
        "\n",
        "# Salvar os resultados\n",
        "try:\n",
        "    results_df.to_csv(\"comparacao_smote_resultados.csv\", index=False)\n",
        "    print(\"Resultados salvos em 'comparacao_smote_resultados.csv'\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao salvar resultados: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN40+X/3/p0xxEKIUO1GTOY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}